{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c2de5d48-1c3a-425b-b966-36a33a3d437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from seaborn) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adina\\anaconda3\\envs\\m\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/294.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/294.9 kB 388.9 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/294.9 kB 930.9 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 174.1/294.9 kB 952.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/294.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.9/294.9 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d87177-c0bf-421f-abfb-4bfdefe037d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "import keras \n",
    "from PIL import Image\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Flatten, Lambda, BatchNormalization,AveragePooling2D \n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37369e2-018b-4389-b607-99c47ee6408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "df = pd.read_csv('train_chestXRAY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ae344d-575f-4ff7-8def-a9ac052ef736",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    'PNEUMONIA' : 0,\n",
    "    'NORMAL':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f23c013-5f30-4bdf-b021-81f4323fe377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = r'C:\\Users\\adina\\Desktop\\תקיית_עבודות\\chest_xray\\train'\n",
    "validaion_ds_path = r\"C:\\Users\\adina\\Desktop\\תקיית_עבודות\\chest_xray\\val\"\n",
    "test_ds_path = r\"C:\\Users\\adina\\Desktop\\תקיית_עבודות\\chest_xray\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90f73a2-239a-46c8-9b3f-e43391b2f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "class ImageProcessor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_array_from_choices(img_array):\n",
    "        options = [1, 2, 3]        \n",
    "        choice = random.sample(options, 2)\n",
    "        if choice == [1, 2]:\n",
    "            img_array = ImageProcessor.increase_contrast(img_array)\n",
    "            img_array = ImageProcessor.sharpened_image(img_array)\n",
    "        elif choice == [1, 3]:\n",
    "            img_array = ImageProcessor.increase_contrast(img_array)\n",
    "        else:\n",
    "            img_array = ImageProcessor.sharpened_image(img_array)\n",
    "        return img_array\n",
    "    \n",
    "    @staticmethod\n",
    "    def crop_image(img):\n",
    "        w, h = img.size\n",
    "        size = min(w, h)\n",
    "        x = (w - size) // 2\n",
    "        y = (h - size) // 2\n",
    "        img = img.crop((x, y, x + size, y + size))\n",
    "        return img_to_array(img)\n",
    "        \n",
    "    @staticmethod\n",
    "    def increase_contrast(img_array, factor=1.5):\n",
    "        # Apply contrast stretching\n",
    "        img_array = (img_array - img_array.min()) / (img_array.max() - img_array.min()) * 255.0\n",
    "    \n",
    "        # Increase the contrast by multiplying pixel values with a factor\n",
    "        img_array = img_array * factor\n",
    "    \n",
    "        # Clip pixel values to be in the valid range [0, 255]\n",
    "        img_array = np.clip(img_array, 0, 255)\n",
    "    \n",
    "        return img_array\n",
    "    \n",
    "    @staticmethod\n",
    "    def sharpened_image(img_array):\n",
    "        kernel = np.array([[[0, -1, 0],\n",
    "                            [-1, 5, -1],\n",
    "                            [0, -1, 0]]], dtype=np.float32)\n",
    "        sharpened_img = convolve(img_array, kernel)\n",
    "        return sharpened_img\n",
    "\n",
    "    @staticmethod\n",
    "    def adaptive_thresholding(img_array):\n",
    "        img_array = np.uint8(img_array)\n",
    "        channels = cv2.split(img_array)\n",
    "        \n",
    "        max_output_value = 255\n",
    "        neighborhood_size = 99\n",
    "        subtract_from_mean = 10\n",
    "        image_binarized = [cv2.adaptiveThreshold(channel,\n",
    "                                max_output_value,\n",
    "                                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY,\n",
    "                                neighborhood_size,\n",
    "                                subtract_from_mean) for channel in channels]\n",
    "        image_binarized = cv2.merge(image_binarized)\n",
    "        return image_binarized\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_image(img_path, target_size=None):\n",
    "        samples = []\n",
    "        # regular image\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        original_img_arr = img_to_array(img)\n",
    "        samples.append(original_img_arr)\n",
    "        # After croping image \n",
    "        img_arr_after_crop = ImageProcessor.crop_image(img)\n",
    "        samples.append(img_arr_after_crop)\n",
    "        # After sharpening image \n",
    "        img_array_after_sharp = ImageProcessor.sharpened_image(original_img_arr)\n",
    "        samples.append(img_array_after_sharp)\n",
    "        # After increasing contrast image \n",
    "        img_arr_after_increase_contrast = ImageProcessor.increase_contrast(original_img_arr)\n",
    "        samples.append(img_arr_after_increase_contrast)\n",
    "        # After croping and another filters \n",
    "        samples.append(ImageProcessor.increase_contrast(img_arr_after_crop))\n",
    "        samples.append(ImageProcessor.sharpened_image(img_arr_after_crop))\n",
    "\n",
    "        # adaptive thresholding images \n",
    "        samples.append(ImageProcessor.adaptive_thresholding(img_arr_after_crop))\n",
    "        samples.append(ImageProcessor.adaptive_thresholding(original_img_arr))\n",
    "        samples.append(ImageProcessor.adaptive_thresholding(img_arr_after_increase_contrast))\n",
    "        samples.append(ImageProcessor.adaptive_thresholding(img_array_after_sharp))\n",
    "        samples.append(ImageProcessor.adaptive_thresholding(img_array_after_sharp))\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165c8925-47a0-45e4-8113-445d5a37799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_df_from_url(ds_path, csv_filename, to_csv=True):\n",
    "    images = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for main_dir in os.listdir(ds_path):\n",
    "        main_dir_path = os.path.join(ds_path, main_dir)\n",
    "        if os.path.isdir(main_dir_path):\n",
    "            for sub_dir_name in os.listdir(main_dir_path):\n",
    "                img_dir_path = os.path.join(main_dir_path, sub_dir_name)\n",
    "                filtered_variations_image = ImageProcessor.preprocess_image(img_dir_path, target_size=(32, 32))\n",
    "\n",
    "                if filtered_variations_image is not None:\n",
    "                    label = [value for key, value in LABELS.items() if key in LABELS.keys() and main_dir in key]\n",
    "                    images.append({\n",
    "                        'filename': sub_dir_name,\n",
    "                        'Path': img_dir_path,\n",
    "                        'label': label[0] if label else None\n",
    "                    })\n",
    "                    \n",
    "                    for i in range(len(filtered_variations_image)):\n",
    "                        X.append(filtered_variations_image[i])\n",
    "                        y.append(label[0])\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(images)\n",
    "    if to_csv:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "    return df, np.array(X).astype('float32') / 255. , np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6196e529-6f52-4fe4-a801-da5caf4814e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, X_train, y_train = generate_df_from_url(train_ds_path, csv_filename = 'train_chestXRAY.csv')\n",
    "validation_df, X_val, y_val = generate_df_from_url(validaion_ds_path, csv_filename = 'validation_chestXRAY.csv')\n",
    "test_df, X_test , y_test = generate_df_from_url(test_ds_path, csv_filename= 'test_chestXRAY.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550a2f34-76c3-43b3-8d97-76ba6635f1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57376, 32, 32, 3)\n",
      "(57376,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db873963-860c-4f70-9f91-bd2262a6233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Dimentionallity was: (57376, 32, 32, 3) for X_train and (6864, 32, 32, 3) for X_test.\n",
      "Dimentionallity reduction to: (57376, 32, 32, 1) for X_train and (6864, 32, 32, 1) for X_test.\n"
     ]
    }
   ],
   "source": [
    "print(f'Previous Dimentionallity was: {X_train.shape} for X_train and '\n",
    "      f'{X_test.shape} for X_test.')\n",
    "X_train = np.mean(X_train, axis=-1, keepdims=True)\n",
    "X_test = np.mean(X_test, axis=-1, keepdims=True)\n",
    "print(f'Dimentionallity reduction to: {X_train.shape} for X_train and '\n",
    "      f'{X_test.shape} for X_test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab2d4b92-10d6-48b1-a111-720caeb9b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2DTranspose,Dropout, Layer, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Flatten, BatchNormalization, Lambda, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import pdb\n",
    "\n",
    "def AutoEncoder(input_shape):\n",
    "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    # Encoder\n",
    "    # first layer\n",
    "    x = Conv2D(128,(3,3) , strides =2 ,activation = 'relu',padding='same')(encoder_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # second layer\n",
    "    x = Conv2D(128, (3, 3),  strides =2 ,activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # third layer \n",
    "    x = Conv2D(128, (3, 3),  strides =2 ,activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # fourth layer \n",
    "    x = Conv2D(128, (3, 3),  strides =2 ,activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    shape_before_flattening = K.int_shape(x)[1:]\n",
    "    x = Flatten()(x)\n",
    "    encoder_output = Dense(units = 2 , name = 'encoder_output')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input = Input(shape =(2,), name='decoder_input')\n",
    "    x= Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    x = Reshape((shape_before_flattening))(x)\n",
    "\n",
    "    # The Reversed layers \n",
    "    # first layer\n",
    "    x = Conv2DTranspose(filters = 128, kernel_size = (3,3) , strides =2 , \n",
    "                    activation = 'relu' , padding= 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    \n",
    "    #second layer \n",
    "    x = Conv2DTranspose(filters = 128, kernel_size = (3,3) , strides =2 , \n",
    "                    activation = 'relu' , padding= 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    # third layer \n",
    "    x = Conv2DTranspose(filters = 128, kernel_size = (3,3) , strides =2 , \n",
    "                    activation = 'relu' , padding= 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    # fourth layer \n",
    "    x = Conv2DTranspose(filters = 128, kernel_size = (3,3) , strides =2 , \n",
    "                    activation = 'relu' , padding= 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.01)(x)\n",
    "    # output layer \n",
    "    decoder_output = Conv2DTranspose(filters = 1, kernel_size = (3,3) , \n",
    "                                        strides=1, \n",
    "                                        activation ='sigmoid',\n",
    "                                        padding = 'same',\n",
    "                                        name = 'decoder_output')(x)\n",
    "    \n",
    "    # Models\n",
    "    encoder = Model(encoder_input,encoder_output)\n",
    "    decoder = Model(decoder_input, decoder_output)\n",
    "    autoencoder = Model(encoder_input , decoder(encoder_output))\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb1efb19-6bbf-4381-a19f-9653eb644416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER:\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 128)       1280      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 447,106\n",
      "Trainable params: 446,082\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "DECODER:\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1536      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 128)        147584    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " decoder_output (Conv2DTrans  (None, 32, 32, 1)        1153      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 597,121\n",
      "Trainable params: 595,073\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "AUTOENCODER:\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 128)       1280      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 1026      \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 32, 32, 1)         597121    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,044,227\n",
      "Trainable params: 1,041,155\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder, decoder = AutoEncoder(input_shape=(32, 32, 1))\n",
    "print('ENCODER:')\n",
    "print(encoder.summary())\n",
    "print('DECODER:')\n",
    "print(decoder.summary())\n",
    "print('AUTOENCODER:')\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "496c6195-0089-489b-b687-ff90b86a58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('autoencoder_weights.h5')\n",
    "autoencoder.save('autoencoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da7d2d0-21de-400c-ab98-25f0d11d8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "574/574 [==============================] - 236s 407ms/step - loss: 0.0817 - val_loss: 0.0915\n",
      "Epoch 2/5\n",
      "574/574 [==============================] - 253s 440ms/step - loss: 0.0762 - val_loss: 0.0741\n",
      "Epoch 3/5\n",
      "574/574 [==============================] - 240s 417ms/step - loss: 0.0752 - val_loss: 0.0741\n",
      "Epoch 4/5\n",
      "574/574 [==============================] - 230s 401ms/step - loss: 0.0745 - val_loss: 0.0738\n",
      "Epoch 5/5\n",
      "574/574 [==============================] - 227s 396ms/step - loss: 0.0742 - val_loss: 0.0735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ff83863b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train,\n",
    "                X_train,\n",
    "                epochs = 5,\n",
    "                batch_size =100,\n",
    "                shuffle = True,\n",
    "                validation_data =(X_test,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ead67464-c8a5-4901-b0ec-2fe05f7b79ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder weights saved successfully.\n"
     ]
    }
   ],
   "source": [
    "weights_file_path = 'autoencoder_weights.h5'\n",
    "autoencoder.save_weights(weights_file_path)\n",
    "print(\"Autoencoder weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf675f0b-ed74-4d7d-988c-0a55c1641593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 566ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 32, 32, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_test = pd.read_csv('test_chestXRAY.csv')\n",
    "test_img_path = df_test['Path'].iloc[1]\n",
    "\n",
    "test_img = ImageProcessor.preprocess_image(test_img_path)\n",
    "test_img = np.array(test_img)\n",
    "\n",
    "index = random.randint(0, test_img.shape[0] -1)\n",
    "\n",
    "img_observation = test_img[index]\n",
    "img = Image.fromarray(img_observation.squeeze().astype('uint8'))\n",
    "img = img.resize((32, 32))\n",
    "img = np.array(img)\n",
    "img = np.mean(img, axis=-1, keepdims=True)\n",
    "img = img / 255.\n",
    "img = img.reshape((1, 32, 32, 1))\n",
    "\n",
    "prediction = autoencoder.predict(img)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79bb9004-64f3-45e2-8927-6059dc70c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_img_array = prediction[0] * 255. \n",
    "reconstructed_sharpened_image = ImageProcessor.sharpened_image(reconstructed_img_array)\n",
    "Image.fromarray(reconstructed_sharpened_image.squeeze().astype('uint8')).resize((32, 32)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74774d55-db13-45da-8f71-ecc7f6366c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57376, 32, 32)\n",
      "(57376, 10, 10)\n",
      "Number of features from 10X10 image: (57376, 100)\n",
      "The shape after applying PCA is -> (57376, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# data preprocessing\n",
    "X_train_2_dim = [img[:, :, 0] for img in X_train]\n",
    "X_train_2_dim = np.array(X_train_2_dim)\n",
    "print(X_train_2_dim.shape)\n",
    "\n",
    "# First try- resizing the image to 10X10.\n",
    "X_train_resized = [cv2.resize(image, (10, 10)) for image in X_train]\n",
    "X_train_resized = np.array(X_train_resized)\n",
    "print(X_train_resized.shape)\n",
    "X_train_resized = X_train_resized.reshape(X_train_resized.shape[0], -1)\n",
    "print(f'Number of features from 10X10 image: {X_train_resized.shape}')\n",
    "\n",
    "# Second try- Reduce the dimention from the original size 32X32..\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "X_train_reshaped = X_train_2_dim.reshape(-1, 32*32)\n",
    "pca = PCA(n_components=50)\n",
    "X_train_pca = pca.fit_transform(X_train_reshaped)\n",
    "print(f'The shape after applying PCA is -> {X_train_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b98d41-5b05-48d6-a6f5-33c7cecc05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_resized_1010_image = KNeighborsClassifier(n_neighbors = 5 , metric = 'minkowski', p=2)\n",
    "classifier_pca_image = KNeighborsClassifier(n_neighbors = 5 , metric = 'minkowski', p=2)\n",
    "regulaer_images_format_classifier = KNeighborsClassifier(n_neighbors = 5 , metric = 'minkowski', p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d328e0b-70c5-42dc-9384-77eeb58db631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_2dim_reshaped after reshaping -> (57376, 1024)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the classifier \n",
    "X_train_2dim_reshaped = X_train_2_dim.reshape(X_train_2_dim.shape[0], -1)\n",
    "print(f'Shape of X_train_2dim_reshaped after reshaping -> {X_train_2dim_reshaped.shape}')\n",
    "\n",
    "classifier_resized_1010_image.fit(X_train_resized, y_train)\n",
    "classifier_pca_image.fit(X_train_pca,y_train)\n",
    "regulaer_images_format_classifier.fit(X_train_2dim_reshaped,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f91bba8-15a9-4790-9e6c-a1e140c5d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test_2dim: (6864, 32, 32)\n",
      "Shape of X_test_2dim after reshaping: (6864, 1024)\n",
      "Shape of X_test_resized (6864, 10, 10)\n",
      "ACCURACY SCORE FOR IMAGES 32X32 ORIGINAL SIZE AS FEATURES: 0.7513111888111889\n",
      "ACCURACY SCORE FOR IMAGES 10X10 SIZE AS FEATURES: 0.7491258741258742\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.756993006993007\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the two models \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_test_2dim = [img[:, :, 0] for img in X_test]\n",
    "X_test_2dim = np.array(X_test_2dim)\n",
    "print(f'Shape of X_test_2dim: {X_test_2dim.shape}')\n",
    "X_test_2dim_reshaped = X_test_2dim.reshape(X_test_2dim.shape[0],-1)\n",
    "print(f'Shape of X_test_2dim after reshaping: {X_test_2dim_reshaped.shape}')\n",
    "\n",
    "X_test_resized = [cv2.resize(image, (10, 10)) for image in X_test_2dim]\n",
    "X_test_resized = np.array(X_test_resized)\n",
    "print(f'Shape of X_test_resized {X_test_resized.shape}')\n",
    "X_test_resized = X_test_resized.reshape(-1, 100)\n",
    "\n",
    "X_test_reshaped = X_test_2dim.reshape(-1, 32*32)\n",
    "X_test_pca = pca.transform(X_test_reshaped)\n",
    "\n",
    "y_pred_for_resized_images = classifier_resized_1010_image.predict(X_test_resized)\n",
    "y_pred_for_pca_images = classifier_pca_image.predict(X_test_pca)\n",
    "y_pred_for_original_images = regulaer_images_format_classifier.predict(X_test_2dim_reshaped)\n",
    "\n",
    "accuracy_for_original_images = accuracy_score(y_pred_for_original_images,y_test) \n",
    "accuray_for_resized = accuracy_score(y_pred_for_resized_images, y_test)\n",
    "accuray_for_pca = accuracy_score(y_pred_for_pca_images, y_test)\n",
    "\n",
    "print(f'ACCURACY SCORE FOR IMAGES 32X32 ORIGINAL SIZE AS FEATURES: {accuracy_for_original_images}')\n",
    "print(f'ACCURACY SCORE FOR IMAGES 10X10 SIZE AS FEATURES: {accuray_for_resized}')\n",
    "print(f'ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: {accuray_for_pca}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5a7f0dd-b21e-429f-80ec-16191a738154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for 50 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 50)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7613636363636364\n",
      "\n",
      "Time for 60 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 60)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7634032634032634\n",
      "\n",
      "Time for 70 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 70)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7578671328671329\n",
      "\n",
      "Time for 80 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 80)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7619463869463869\n",
      "\n",
      "Time for 90 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 90)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7588869463869464\n",
      "\n",
      "Time for 100 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 100)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7628205128205128\n",
      "\n",
      "Time for 110 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 110)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7628205128205128\n",
      "\n",
      "Time for 120 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 120)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.761509324009324\n",
      "\n",
      "Time for 130 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 130)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7583041958041958\n",
      "\n",
      "Time for 140 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 140)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7607808857808858\n",
      "\n",
      "Time for 150 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 150)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7620920745920746\n",
      "\n",
      "Time for 160 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 160)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7619463869463869\n",
      "\n",
      "Time for 170 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 170)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7581585081585082\n",
      "\n",
      "Time for 180 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 180)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7585955710955711\n",
      "\n",
      "Time for 190 components testing...\n",
      "\n",
      "The shape after applying PCA is -> (57376, 190)\n",
      "Training this classifier...\n",
      "\n",
      "Training Done.\n",
      " Let's evaluate the accuracy: \n",
      "\n",
      "ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: 0.7578671328671329\n",
      "Best Accuracy from loop: 0.7634032634032634 with n_components: 60\n",
      "Saving the Classifier...\n",
      "Successfully saved the Classifier!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "best_accuracy = accuray_for_pca\n",
    "best_n_components = 50\n",
    "best_pca_for_classifier = None\n",
    "classifier = None\n",
    "for i in range(50, 200 , 10):\n",
    "    print(f'\\nTime for {i} components testing...\\n')\n",
    "    pca = PCA(n_components=i)\n",
    "    X_train_pca = pca.fit_transform(X_train_reshaped)\n",
    "    print(f'The shape after applying PCA is -> {X_train_pca.shape}')\n",
    "\n",
    "    print('Training this classifier...\\n')\n",
    "    classifier_pca_image = KNeighborsClassifier(n_neighbors = 5 , metric = 'minkowski', p=2)\n",
    "    classifier_pca_image.fit(X_train_pca, y_train)\n",
    "    print(\"Training Done.\\n Let's evaluate the accuracy: \\n\")\n",
    "\n",
    "    X_test_reshaped = X_test_2dim.reshape(-1, 32*32)\n",
    "    X_test_pca = pca.transform(X_test_reshaped)\n",
    "    y_pred_for_pca_images = classifier_pca_image.predict(X_test_pca)\n",
    "    accuracy_for_pca = accuracy_score(y_pred_for_pca_images, y_test)\n",
    "    print(f'ACCURACY SCORE FOR IMAGES 32X32 SIZE AFTER PCA AS FEATURES: {accuracy_for_pca}')\n",
    "\n",
    "    if accuracy_for_pca > best_accuracy:\n",
    "        best_pca = pca\n",
    "        best_accuracy = accuracy_for_pca\n",
    "        best_n_components = i\n",
    "        classifier = classifier_pca_image\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f'Best Accuracy from loop: {best_accuracy} with n_components: {best_n_components}')\n",
    "print('Saving the Classifier...')\n",
    "\n",
    "import joblib\n",
    "classifier_file_path = 'KNN_classifier.pkl'\n",
    "joblib.dump(classifier, classifier_file_path)\n",
    "print('Successfully saved the Classifier!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18579eb4-cdd1-4a4a-bfa6-f2fea664e19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdc82278-3faa-48ac-b5dc-0d1b070730b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make a single prediction using transfer learning.\n",
    "    For this test, I didn't use the validation set and I will make the\n",
    "    testing on this set. (validation_df, X_val, y_val)\n",
    "\"\"\"\n",
    "\n",
    "test_images_as_dict = validation_df.to_dict()\n",
    "del test_images_as_dict['filename']\n",
    "\n",
    "images_in_the_right_format_dict = {}\n",
    "for i, (path_item, label_item) in enumerate(zip(test_images_as_dict['Path'].items(), test_images_as_dict['label'].items())):\n",
    "    path_key, path_value = path_item\n",
    "    label_key, label_value = label_item\n",
    "\n",
    "    images_in_the_right_format_dict[i] = {\n",
    "        'image path': path_value,\n",
    "        'label index': label_value,\n",
    "        'label name' : [key for key,value in LABELS.items() if value == label_value][0]\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94374bbe-8f23-4a39-b5c8-d29be65c374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_images is (16, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the dictionary\n",
    "test_images = []\n",
    "for key, value in images_in_the_right_format_dict.items():\n",
    "    img = Image.open(value['image path'])\n",
    "    img = img.resize((32,32))\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array.astype('float32')/ 255.\n",
    "    img_array = np.mean(img_array, axis=-1, keepdims=True)\n",
    "    \n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "print(f'Shape of test_images is {test_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b2e00e2-9559-479e-b1d0-b300b7d9b261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e8854b7-2594-4d47-96eb-e1310b0de059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 1, 1024)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Start the main loop to get the predictions.\n",
    "predictions = []\n",
    "choice = None\n",
    "for i, image in enumerate(test_images):\n",
    "    image = image.reshape((1, 32, 32, 1))\n",
    "    ae_recontruction_prediction = autoencoder.predict(image)\n",
    "    real_image_size = (ae_recontruction_prediction.squeeze() * 255.).astype('uint8')\n",
    "    ae_recontruction_prediction = ae_recontruction_prediction.reshape(-1, 32*32)\n",
    "    predictions.append(ae_recontruction_prediction)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20178a9b-0558-4d95-95ee-aa46ba8ac565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 60)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_for_knn = []\n",
    "for prediction in predictions:\n",
    "    observation = best_pca.transform(prediction)\n",
    "    observations_for_knn.append(observation)\n",
    "\n",
    "observations_for_knn = np.array(observations_for_knn)\n",
    "observations_for_knn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84ac873a-2623-40c3-8092-eda5ea019d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i,observation in enumerate(observations_for_knn):\n",
    "    y_pred = classifier.predict(observation)\n",
    "    prediction_label = y_pred[0]\n",
    "    print(prediction_label == y_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4aa10223-4cd9-43d0-aceb-5bc0412d55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now, I want to try to make predictions in X_val as well.\n",
    "\"\"\"\n",
    "predictions_from_Xval = []\n",
    "for image in X_val:\n",
    "    image = np.mean(image, axis=-1, keepdims=True)\n",
    "    image = image.reshape((1, 32, 32, 1))\n",
    "    ae_recontruction_prediction = autoencoder.predict(image)\n",
    "    predictions_from_Xval.append(ae_recontruction_prediction.reshape(-1, 32*32))\n",
    "\n",
    "predictions_from_Xval = np.array(predictions_from_Xval)\n",
    "\n",
    "observations_for_knn_from_Xval = []\n",
    "for prediction in predictions_from_Xval:\n",
    "    observation = best_pca.transform(prediction)\n",
    "    observations_for_knn_from_Xval.append(observation)\n",
    "\n",
    "observations_for_knn_from_Xval = np.array(observations_for_knn_from_Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "47cf0808-1c5f-47e4-bf05-dc16523b631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 1, 60)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_for_knn_from_Xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1eef9ad0-f4d0-492d-b3d2-b14b45c8566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_for_knn_from_Xval_reshaped = observations_for_knn_from_Xval.reshape(176, 60)\n",
    "y_pred = classifier.predict(observations_for_knn_from_Xval_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "46d386b8-d144-4ebb-889d-8c107ee582d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6988636363636364\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy Score: {accuracy}')\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746b4b5-760f-463a-9a6b-2dc877577c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28296fe4-bedf-45b6-956d-f7c44d21258d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
